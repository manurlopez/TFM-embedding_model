{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo realizado con *embeddings* para o TFM **\"Estimación automática de signos de depresión a partir de análises de texto.\"** do Máster universitario en tecnoloxías de análise de datos masivos: Big Data no curso académico 2019/2020\n",
    "\n",
    "## Autor: Manuel Ramón Varela López"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ver **Readme** para as instruccións de uso. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos previos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Comezamos cos datos de configuración do notebook.** \n",
    "    - Indicamos o ficherio JSON cas preguntas e respostas do test BDI.\n",
    "    - Indicamos o directorio onde se gardan os documentos XML cas publicacións dos usuarios.\n",
    "    - Indicamos o ficheiro que contén os resultados reais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_file = \"inquerito.json\"\n",
    "dir_corpus = 'corpus'\n",
    "file_real_results = 'Depression Questionnaires_anon.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Improtamos as librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import nltk\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Configuración**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comezo do script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Cargamos as preguntas.** \n",
    "    - Cargamos as preguntas do test BDI do arquivo indicado ao comezo do script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(questions_file) as json_file:\n",
    "    questions = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Consultamos o número de xml cás publicacións de Reddit**.\n",
    "    - Collemos todos os arquivos XML (un para cada usuario) no directorio na que se gardan estes arquivos indicado ao comezo do script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_files = [f for f in os.listdir(dir_corpus) if os.path.isfile(os.path.join(dir_corpus, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Lemos os arquivos XML**.\n",
    "    - Preocesamos esos arquivos e gardámola información toda nun dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for file in corpus_files:\n",
    "    dataElement = {}\n",
    "    writings = []\n",
    "    tree = ET.parse(dir_corpus + os.path.sep + file)\n",
    "    root = tree.getroot()\n",
    "    for child in root:\n",
    "        if child.tag == 'ID':\n",
    "            dataElement['id']=child.text\n",
    "        elif child.tag=='WRITING':\n",
    "            writing = {}\n",
    "            for wriIter in child:\n",
    "                if wriIter.tag == 'TITLE':\n",
    "                    writing['title']=wriIter.text\n",
    "                elif wriIter.tag == 'DATE':\n",
    "                    writing['date']=wriIter.text\n",
    "                elif wriIter.tag == 'INFO':\n",
    "                    writing['info']=wriIter.text\n",
    "                elif wriIter.tag == 'TEXT':\n",
    "                    writing['text']=wriIter.text\n",
    "            writings.append(writing)\n",
    "    dataElement['corpus'] = writings\n",
    "    data.append(dataElement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Cargamos o modelo pre-adestrado**.\n",
    "    - Se non está dispoñible, descárgase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Con este modelo\n",
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Procesamos o corpus de cada usuario**\n",
    "    - Convertimos o texto e o título das publicacións dos usuarios en arrays de palabras.\n",
    "    - Eliminamos as stop-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "for user in data:\n",
    "    corpus = user.get(\"corpus\")\n",
    "    for document in corpus:\n",
    "        #Quitamos as stop words do titulo\n",
    "        title = document.get(\"title\")\n",
    "        document['title'] = [word for word in gensim.utils.simple_preprocess(str(title)) if word not in stop_words]\n",
    "        text = document.get(\"text\")\n",
    "        document['text'] = [word for word in gensim.utils.simple_preprocess(str(text)) if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Construimos o array de palabras co que se calculará a similitude co corpus do usuario para cada pregunta**\n",
    "    - Recibimos un texto cas palabras que se van calcular as similitudes.\n",
    "    - Devolvemos un array de palabras eliminando as stopWords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_query(texto):\n",
    "    q_aux = [word for word in gensim.utils.simple_preprocess(texto) if word not in stop_words]\n",
    "    return q_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Calculo de similitude entre dúas palabras**\n",
    "    - Calculamos a similitude entre dúas palabras.\n",
    "    - Cada palabra é representada por un vector.\n",
    "    - A similitude é o coseno entre os vectores de cada palabra.\n",
    "    - Se algunha das palabras non está no modelo, devolvemos **None**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosSim(word1,word2):\n",
    "    if (word1 in model) and (word2 in model):\n",
    "        return model.similarity(word1, word2)\n",
    "    return None    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Creamos un DataFrame para gardar os resultados.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recorremos todas as preguntas\n",
    "d = ['subject']\n",
    "for question in questions:\n",
    "    d.append(question['question_number'])\n",
    "\n",
    "#Creamos os dataframes para as medidas\n",
    "results = pd.DataFrame(columns=d)\n",
    "#for i in range(medidas):\n",
    "    #aux = \n",
    "    #results.append(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Realizamos o cálculo das similitudes entre pregunta e resposta máis alta e o corpus do usuario**\n",
    "    - Para cada palabra do array cas palabras de cada pregunta e a resposta máis alta calculamos a similitude con cada palabra do corpus do usuario.\n",
    "    - Unha vez que teñamos todas as similitudes, calculamos a medida de similitude de todas as similitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recorremos todos os usuarios\n",
    "for user in data:\n",
    "    \n",
    "    #Collemos o id do usuario\n",
    "    subject = user['id']\n",
    "    \n",
    "    #Imos gardando as medidas para cada usuario\n",
    "    subject_res = {'subject':subject}\n",
    "    #subject_res.append()\n",
    "    \n",
    "    #Collemos as preguntas\n",
    "    for question in questions:\n",
    "        #print(question)\n",
    "        \n",
    "        #Collemos o texto da pregunta e o numero\n",
    "        question_text = question['question_text']\n",
    "        question_number = question['question_number']\n",
    "        \n",
    "        #Collemos a ultima resposta\n",
    "        answers = question['answers']\n",
    "        answer = answers[len(answers)-1]\n",
    "        answer_text = answer['answer_text']\n",
    "        \n",
    "        #Collemos cada palabra da pregunta\n",
    "        question_array = build_query(question_text + \" \" + answer_text)\n",
    "        simCoseno = []\n",
    "        for word_question in question_array:\n",
    "            \n",
    "            #Collemos o corpus do usuario\n",
    "            corpus = user.get(\"corpus\")\n",
    "            \n",
    "            #Recorremos todas os textos das palabras\n",
    "            for coment in corpus:\n",
    "                \n",
    "                #Recorremos todas as palabras do titulo\n",
    "                for word_title in coment.get('title'):\n",
    "                    \n",
    "                    simCos = cosSim(word_question,word_title)\n",
    "                    if simCos is not None:\n",
    "                        simCoseno.append(simCos)   \n",
    "                \n",
    "                #Recorremos todas as palabras do text\n",
    "                for word_text in coment.get('text'):\n",
    "\n",
    "                    simCos = cosSim(word_question,word_text)\n",
    "                    if simCos is not None:\n",
    "                        simCoseno.append(simCos)\n",
    "        \n",
    "        #Tras calcular todas as similirades entre palabras da pregunta e dos textos calculamos o medio\n",
    "        score = np.mean(simCoseno)\n",
    "        subject_res[question_number] = score\n",
    "        \n",
    "    #Gardamos os scores para cada usuario\n",
    "    results = results.append(subject_res,ignore_index=True)\n",
    "    \n",
    "    #Indicamos que un usuario rematou de ler\n",
    "    print(\"User \" + subject + \" finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Mostramos os resultados obtidos**\n",
    "    - Indicamos que a columna \"subject\" é índice do DataFrame.\n",
    "    - Mostramos o data frame, para cada usuario e pregunta indicamos a similitude entre a pregunta e a resposta máis alta e o corpus do usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=results.set_index('subject')\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Normalización**\n",
    "    - Normalizamos cada resposta a cada pregunta de cada usuario a un valor entre 0 e 1.\n",
    "    - Despois multiplicamos polo valor do número de preguntas que ten a resposta menos 1.\n",
    "    - Calculamos o floor para obter un número enteiro.\n",
    "    - Ese valor e a resposta da pregunta. Terá que estar entre 0 e 3 para as preguntas con 4 respostas e 0 e 6 para as preguntas de 7 respostas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    q = question['question_number']\n",
    "    cats = len(question['answers'])\n",
    "\n",
    "    #Results1\n",
    "    if (max(results[q])-min(results[q])) > 0:\n",
    "        results[q] = np.floor((results[q]-min(results[q]))/(max(results[q])-min(results[q]))*(cats-1))\n",
    "    results[q] = pd.to_numeric(results[q],downcast='integer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Cambiamos a numeración das preguntas que teñen 7 respostas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for aux2 in [16,18]:\n",
    "    results.loc[results[aux2] == 1,aux2] = '1a'\n",
    "    results.loc[results[aux2] == 2,aux2] = '1b'\n",
    "    results.loc[results[aux2] == 3,aux2] = '2a'\n",
    "    results.loc[results[aux2] == 4,aux2] = '2b'\n",
    "    results.loc[results[aux2] == 5,aux2] = '3a'\n",
    "    results.loc[results[aux2] == 6,aux2] = '3b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Mostramos as respostas de cada usuario para cada unha das preguntas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Cargamos os datos reais e gardámolos nun DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_results = pd.read_csv(file_real_results,index_col=False,header=None,sep='\\t')\n",
    "real_results.columns = d\n",
    "real_results=real_results.set_index('subject')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Mostramos os resultados reais**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(real_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Funcións necesarias para o calculo dos resultados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necesitamos esta función porque algunhas preguntas teñen letra\n",
    "def transform(answer):\n",
    "    if(type(answer)!=str):\n",
    "        return answer\n",
    "    elif len(answer) == 1:\n",
    "        return int(answer)\n",
    "    elif len(answer) == 2:\n",
    "        return int(answer[0])\n",
    "\n",
    "def dhcl_score(real,estimed):\n",
    "    if (real <= 9) and (estimed<=9):\n",
    "        return 1\n",
    "    elif (real>29) and (estimed>29):\n",
    "        return 1\n",
    "    elif (real>9) and (real<=18) and (estimed>9) and (estimed<=18):\n",
    "        return 1\n",
    "    elif (real>18) and (real<=29) and (estimed>18) and (estimed<=29):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Calculamos as 4 medidas de avaliación descritas no TFM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mad = 3\n",
    "score_array = []\n",
    "\n",
    "#Recorremos todos os usuarios\n",
    "for identificador in real_results.index.values:\n",
    "    hits = 0\n",
    "    crs = 0\n",
    "    scr_real = 0\n",
    "    scr_stm = 0\n",
    "    #Recorremos todas as preguntas\n",
    "    for question in questions:\n",
    "        q = question['question_number']\n",
    "        real_a = real_results.loc[identificador,q]\n",
    "        estimated_a = results.loc[identificador,q]\n",
    "\n",
    "        #Contamos as pregutnas acertadas\n",
    "        if real_a == estimated_a:\n",
    "            hits = hits + 1\n",
    "\n",
    "        #Contamos camo de cerca estamos\n",
    "        crs_aux = (mad - abs(transform(real_a)-transform(estimated_a)))/mad\n",
    "        crs = crs + crs_aux\n",
    "\n",
    "        #Calculamos os valores de depresion\n",
    "        scr_real = scr_real + transform(real_a)\n",
    "        scr_stm = scr_stm + transform(estimated_a)\n",
    "\n",
    "    #Calculamos o porcentaxe de preguntas acertadas\n",
    "    hit_score_aux = hits / len(questions)\n",
    "    cls_score_aux = crs / len(questions)\n",
    "    dl = (63 - abs(scr_real - scr_stm))/63\n",
    "    dhcl = dhcl_score(scr_real,scr_stm)\n",
    "    score_array.append({'subject':identificador,\n",
    "                        'hit rate score':hit_score_aux,\n",
    "                        'closeness rate score':cls_score_aux,\n",
    "                        'real score':scr_real,\n",
    "                        'estimated_score':scr_stm,\n",
    "                        'dl':dl,\n",
    "                        'dchr':dhcl})\n",
    "\n",
    "\n",
    "score = pd.DataFrame(score_array)\n",
    "score = score.set_index('subject')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Mostramos as 4 medidas de avaliación para cada un dos suxeitos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Mostramos as medidas de avaliación do modelo proposto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(score[['hit rate score','closeness rate score','dl','dchr']].describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
